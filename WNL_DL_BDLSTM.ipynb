{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-03 21:18:48.915200: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-03 21:18:48.992567: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-04-03 21:18:49.495146: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-03 21:18:49.495192: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-03 21:18:49.495196: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections.abc import Iterable\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "import time\n",
    "from keras.utils import plot_model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# define gvars\n",
    "SEED = 1080\n",
    "VCHUNK = 64\n",
    "NUM_CLASSES = 12\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "SPLIT = 0.2\n",
    "EPOCHS = 50\n",
    "LR = 0.001\n",
    "METRIC = 'val_accuracy'\n",
    "MIN_DELTA = 1e-4\n",
    "PATIENCE = 50\n",
    "THRESHOLD = 0.5\n",
    "\n",
    "# Set random seed\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# import from csv to dataframe\n",
    "df = pd.read_csv('wnl/WNL_TLS_Dataset_ECH.csv', sep='\\t')\n",
    "esni = pd.read_csv('wnl/WNL_TLS_Dataset_ESNI.csv', sep='\\t')\n",
    "\n",
    "# set checkpoint path and filename\n",
    "check_name = \"WNL_CLASS_BDLSTM.chkp\"\n",
    "check_path = \"checkpoint/\" + check_name\n",
    "\n",
    "# define our callbacks\n",
    "callbacks = [\n",
    "\n",
    "    # define checkpoint callback\n",
    "    tf.keras.callbacks.ModelCheckpoint (\n",
    "        check_path,\n",
    "        monitor= 'val_accuracy',\n",
    "        verbose= 1,\n",
    "        save_best_only= True,\n",
    "        save_weights_only= True,\n",
    "        mode= 'auto',\n",
    "        save_freq='epoch',\n",
    "        options=None,\n",
    "        initial_value_threshold=THRESHOLD,\n",
    "    ),\n",
    "\n",
    "    # # define early stopping callback\n",
    "    # tf.keras.callbacks.EarlyStopping(\n",
    "    #     monitor=METRIC,\n",
    "    #     min_delta=MIN_DELTA,\n",
    "    #     patience=PATIENCE,\n",
    "    #     verbose=1\n",
    "    # ),\n",
    "\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''https://tls13.xargs.org/#client-hello/annotated'''\n",
    "\n",
    "def flatten(L):\n",
    "    '''flattens a list of nested list of arbitrary depth into a single concatenated list'''\n",
    "    for x in L:\n",
    "        if isinstance(x, Iterable) and not isinstance(x, (str, bytes)):\n",
    "            yield from flatten(x)\n",
    "        else:\n",
    "            yield x\n",
    "\n",
    "def padding(A, size):\n",
    "    '''pads a given list to a given length'''\n",
    "    t = size - len(A)\n",
    "\n",
    "    return np.pad(A, pad_width=(0, t), mode='constant')\n",
    "\n",
    "def parse_CHP(p):\n",
    "    '''\n",
    "    implements the Shamsimukhametov et al. bytes recomposition algorithm\n",
    "    builds a fixed length feature vector from clienthello message\n",
    "    '''\n",
    "    T = []\n",
    "    L = []\n",
    "    V = []\n",
    "\n",
    "    handshake_len = sum(p[3:5])\n",
    "    clienthello_len = sum(p[6:9])\n",
    "    sid_len = p[43]\n",
    "    i = 44+sid_len\n",
    "\n",
    "    sid = p[44:i]\n",
    "    ciphersuite_len = sum(p[i:i+2])\n",
    "    ciphersuite = p[i+2:i+2+ciphersuite_len]\n",
    "    i += 4 + ciphersuite_len\n",
    "\n",
    "    ext_len = sum(p[i:i+2])\n",
    "    i = i+2\n",
    "    \n",
    "    T.append(padding(sid, 32))\n",
    "    L.append(handshake_len)\n",
    "    L.append(clienthello_len)\n",
    "    L.append(sid_len)\n",
    "    L.append(ciphersuite_len)\n",
    "    L.append(ext_len)\n",
    "    V.append(ciphersuite)\n",
    "    \n",
    "    end = i + ext_len\n",
    "    count = 0   \n",
    "\n",
    "    while (i < len(p)):\n",
    "\n",
    "        n_ext_id = sum(p[i:i+2])\n",
    "        i+=2\n",
    "        \n",
    "        n_ext_len = sum(p[i:i+2])\n",
    "        i+=2\n",
    "\n",
    "        T.append(n_ext_id)\n",
    "        L.append(n_ext_len)\n",
    "        V.append(p[i:i+n_ext_len])\n",
    "        i+=n_ext_len\n",
    "\n",
    "    T = padding(list(flatten(T)),40)\n",
    "    L = padding(list(flatten(L)),13)\n",
    "    V = list(flatten(V))\n",
    "    VC = V[:VCHUNK]\n",
    "\n",
    "    return np.concatenate((T,L,VC))\n",
    "\n",
    "def parse_SHP(p):\n",
    "    '''\n",
    "    implements the Shamsimukhametov et al. bytes recomposition algorithm\n",
    "    builds a fixed length feature vector from serverhello message\n",
    "    '''\n",
    "    T = []\n",
    "    L = []\n",
    "    V = []\n",
    "\n",
    "    handshake_len = sum(p[3:5])\n",
    "    serverhello_len = sum(p[6:9])\n",
    "    sid_len = p[43]\n",
    "    i = 44+sid_len\n",
    "\n",
    "    sid = p[44:i]\n",
    "    ciphersuite = sum(p[i:i+2])\n",
    "    i += 3\n",
    "\n",
    "    ext_len = sum(p[i:i+2])\n",
    "    i = i+2\n",
    "    \n",
    "    T.append(padding(sid, 32))\n",
    "    T.append(ciphersuite)\n",
    "    L.append(handshake_len)\n",
    "    L.append(serverhello_len)\n",
    "    L.append(sid_len)\n",
    "    L.append(ext_len)\n",
    "\n",
    "    end = i + ext_len\n",
    "    count = 0    \n",
    "    \n",
    "    while (i < len(p)):\n",
    "\n",
    "        n_ext_id = sum(p[i:i+2])\n",
    "        i+=2\n",
    "        \n",
    "        n_ext_len = sum(p[i:i+2])\n",
    "        i+=2\n",
    "\n",
    "        T.append(n_ext_id)\n",
    "        L.append(n_ext_len)\n",
    "        V.append(p[i:i+n_ext_len])\n",
    "        i+=n_ext_len\n",
    "\n",
    "    T = padding(list(flatten(T)),40)\n",
    "    L = padding(list(flatten(L)),10)\n",
    "    V = list(flatten(V))\n",
    "    VC = V[:VCHUNK]\n",
    "\n",
    "    return np.concatenate((T,L,VC))\n",
    "\n",
    "def parse_payload(CH,SH):\n",
    "    '''builds a concatenated fixed length feature vector of recomposed bytes'''\n",
    "    chp  = np.fromstring(CH, dtype=int, sep=',')\n",
    "    shp  = np.fromstring(SH, dtype=int, sep=',')\n",
    "    chv = parse_CHP(chp)\n",
    "    shv = parse_SHP(shp)\n",
    "    return np.concatenate((chv,shv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform according to recomposition algorithm\n",
    "df['X'] = df.apply(lambda row : parse_payload(row['ClientHello'], row['ServerHello']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode classes into integers as terget variable vector y\n",
    "labels = df.Label.unique()\n",
    "df['Label'] = df['Label'].astype('category')\n",
    "df['target'] = df['Label'].cat.codes\n",
    "y = df['target'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expand array to proper dims\n",
    "dfx = pd.DataFrame(df['X'].tolist()).add_prefix(\"x\")\n",
    "X = dfx.to_numpy(dtype=np.int16)\n",
    "X = X/256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# split the training data into train and val_test sets using categorical stratification\n",
    "X_train, X_val_test, y_train , y_val_test = train_test_split(   X,\n",
    "                                                                y,\n",
    "                                                                stratify=y,\n",
    "                                                                shuffle=True,\n",
    "                                                                test_size=SPLIT,\n",
    "                                                                random_state=SEED\n",
    "                                                                )\n",
    "\n",
    "# split the val_test data into val and test sets using categorical stratification\n",
    "X_val, X_test, y_val , y_test = train_test_split(       X_val_test,\n",
    "                                                        y_val_test,\n",
    "                                                        test_size=0.5,\n",
    "                                                        random_state=SEED\n",
    "                                                        )\n",
    "# get sizes\n",
    "len_a = len(X_train)\n",
    "len_b = len(X_train[5])\n",
    "\n",
    "# reshape to fit the model\n",
    "X_train = tf.reshape(X_train,(len_a,len_b,1))\n",
    "\n",
    "# check for data leakage\n",
    "print('Data Leak Check')\n",
    "# need method to check for duplicate rows in data\n",
    "print(len(X_train))\n",
    "print(len(X_val))\n",
    "print(len(X_test))\n",
    "print(len(X_train), len(y_train), len(X_val_test), len(y_val_test), len(X_val), len(X_test), len(y_val), len(y_test)), print(len(X_train[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compose the model architecture function\n",
    "def compose_model():\n",
    "    in_layer = layers.Input(shape=(len_b,1))\n",
    "    x = layers.Bidirectional(layers.LSTM(256, return_sequences=True))(in_layer)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPool1D()(x)\n",
    "    x = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPool1D()(x)\n",
    "    x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPool1D()(x)\n",
    "    \n",
    "    # generic stacked dense softmax classifier\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "    x = layers.Dense(len_b, activation='relu')(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    out_layer = layers.Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "    return Model(inputs=in_layer, outputs=out_layer)\n",
    "\n",
    "# instantiate the model\n",
    "model = compose_model()\n",
    "\n",
    "# load previously trained weights\n",
    "#status = model.load_weights(check_path).expect_partial()\n",
    "\n",
    "# inspect model stack\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile and fit the model to the data\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.Adam(learning_rate=LR),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# save model plot\n",
    "plot_model(model, to_file='BDLSTM.png')\n",
    "\n",
    "# start timer\n",
    "start = time.time()\n",
    "# fit the model\n",
    "history = model.fit (   X_train, y_train,\n",
    "                        epochs=EPOCHS,\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        validation_data=(X_val, y_val),\n",
    "                        verbose=1,\n",
    "                        callbacks=callbacks,\n",
    "                        )\n",
    "# stop timer\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on our test data\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = np.rint(y_pred)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the models performance (I hope this isnt garbage)\n",
    "print(classification_report(y_test, y_pred, target_names=labels, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Training Time: \"+str(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('BD-LSTM Model Accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.savefig('bdlstm_acc.jpg')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('BD-LSTM Model Loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.savefig('bdlstm_loss.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
